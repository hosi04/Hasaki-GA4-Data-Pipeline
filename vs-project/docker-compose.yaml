version: '3'

services:
  zookeeper:
    image: wurstmeister/zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    healthcheck:
      test: ["CMD-SHELL", "echo ruok | nc localhost 2181 | grep imok"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - airflow-network
    restart: unless-stopped
      
  kafka:
    image: wurstmeister/kafka
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_ADVERTISED_HOST_NAME: localhost
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
    depends_on:
      zookeeper:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 9092 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    networks:
      - airflow-network
    restart: unless-stopped

  clickhouse:
    image: clickhouse/clickhouse-server:latest
    container_name: clickhouse
    ports:
      - "8123:8123"
      - "9000:9000"
    volumes:
      - clickhouse-data:/var/lib/clickhouse
    environment:
      CLICKHOUSE_DB: default
      CLICKHOUSE_USER: admin
      CLICKHOUSE_PASSWORD: admin
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:8123/ping || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s
    networks:
      - airflow-network
    restart: unless-stopped

  # cube:
  #   image: cubejs/cube:latest
  #   container_name: cube
  #   user: "1001"                      # ðŸ‘ˆ cháº¡y container báº±ng UID 1001
  #   ports:
  #     - "4000:4000"  # Playground UI
  #     - "4001:4001"  # REST API
  #   depends_on:
  #     - clickhouse
  #   environment:
  #     - CUBEJS_DEV_MODE=true
  #     - CUBEJS_API_SECRET=secret123
  #     - CUBEJS_DB_TYPE=clickhouse
  #     - CUBEJS_DB_HOST=clickhouse
  #     - CUBEJS_DB_PORT=8123
  #     - CUBEJS_DB_NAME=tracking_problem_mart
  #     - CUBEJS_DB_USER=default
  #     - CUBEJS_DB_PASS=

  #   volumes:
  #     - ./schema:/cube/conf/model
  #   networks:
  #     - airflow-network

  minio:
    image: minio/minio:RELEASE.2024-06-11T03-13-30Z
    container_name: minio
    ports:
      - "9200:9000"  # Äá»•i tá»« 9100:9000 thÃ nh 9200:9000
      - "9201:9001"  # Web UI
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin123
    volumes:
      - ./minio/data:/data
    command: server --console-address ":9001" /data
    networks:
      - airflow-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped
    networks:
      airflow-network:
        # driver: bridge

  airflow-postgres:
    image: postgres:12
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
      POSTGRES_HOST_AUTH_METHOD: trust
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s
    networks:
      - airflow-network
    restart: unless-stopped

  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile
    depends_on:
      airflow-postgres:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
      AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: 1
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
    user: "50000:50000"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/data:/opt/airflow/data
    command: >
      bash -c "
        echo 'Initializing Airflow database...' &&
        airflow db init &&
        echo 'Creating admin user...' &&
        airflow users create \
          --username airflow \
          --password airflow \
          --firstname Airflow \
          --lastname User \
          --role Admin \
          --email airflow@example.com || echo 'User already exists' &&
        echo 'Airflow initialization completed!'
      "
    networks:
      - airflow-network

  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      kafka:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
      AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: 1
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
      AIRFLOW__WEBSERVER__WEB_SERVER_PORT: 8080
    user: "50000:50000"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/data:/opt/airflow/data
      - ./credentials:/opt/airflow/credentials # <-- thÃªm dÃ²ng nÃ y

      - /home/ngocthanh/Prime/intern/migrate-data-project/pycharm-project:/opt/airflow/spark_project
    ports:
      - "8080:8080"
      - "8001:8001"
    command: webserver
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: "1.0"
    networks:
      - airflow-network
    restart: unless-stopped

  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      kafka:
        condition: service_healthy
      clickhouse:
        condition: service_healthy
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-postgres/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
      AIRFLOW__CORE__MAX_ACTIVE_RUNS_PER_DAG: 1
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
    user: "50000:50000"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./airflow/logs:/opt/airflow/logs
      - ./airflow/data:/opt/airflow/data
      - ./credentials:/opt/airflow/credentials # <-- thÃªm dÃ²ng nÃ y

      - /home/ngocthanh/Prime/intern/migrate-data-project/pycharm-project:/opt/airflow/spark_project
    command: scheduler
    healthcheck:
      test: ["CMD-SHELL", "airflow jobs check --job-type SchedulerJob --hostname airflow-scheduler || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    deploy:
      resources:
        limits:
          memory: 2g
          cpus: "1.0"
    networks:
      - airflow-network
    restart: unless-stopped

  spark-master:
    image: bitnami/spark:latest
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_MASTER_WEBUI_PORT=8081
    ports:
      - "8081:8081"
      - "7077:7077"
    volumes:
      - spark-data:/bitnami
      - ./include:/opt/airflow/include
      - ./apps:/opt/spark-apps
      - ./data:/opt/spark-data


      - /home/ngocthanh/spark-3.5.3/jars:/opt/spark-jars
      - /home/ngocthanh/Prime/intern/migrate-data-project/pycharm-project:/opt/bitnami/spark/spark_project
    networks:
      - airflow-network

  spark-worker:
    image: bitnami/spark:latest
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=2G
      - SPARK_WORKER_CORES=2
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    volumes:
      - spark-data:/bitnami
      - ./include:/opt/airflow/include
      - ./apps:/opt/spark-apps
      - ./data:/opt/spark-data


      - /home/ngocthanh/spark-3.5.3/jars:/opt/spark-jars
      - /home/ngocthanh/Prime/intern/migrate-data-project/pycharm-project:/opt/bitnami/spark/spark_project
    depends_on:
      - spark-master
    networks:
      - airflow-network

networks:
  airflow-network:
    driver: bridge
  

volumes:
  postgres-data:
  zookeeper-data:
  zookeeper-logs:
  kafka-data:
  clickhouse-data:

  spark-data: